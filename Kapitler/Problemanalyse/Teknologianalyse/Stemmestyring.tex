\label{sec:stemmestyring}
Stemmestyrings-controllere er en yderst kompleks teknologi, der første gang blev fremvist af Alexander Graham Bell i året 1870. Alexander G. Bell opdagede en sammenhæng i vores sprog, og hvordan ord og sætninger kunne opdeles i fonemer\footnote{fonem: \textit{Den mindste udtaleenhed i et sprog som ved ombytning med en anden, fungerer betydningsadskillende}\cite{DDOfonem}}. Han ønskede en enhed, der kunne transformere nogle få sætninger til et billede, som han kunne bruge til hans hørehæmmede kone. Paradoksalt nok førte denne idé i stedet til telefonen\cite{SpeechGUIDE}. Sidenhen er der sket meget på området af stemmestyrede controllere. Bell Laboratories og IBM videreudvikledte i året 1950-1960 konceptet om én controller der kunne omsætte analoge stemmer til digitale impulser. I 1970’erne videreudviklede ARPA Speech Understanding Research systemet, hvor de i højere grad fokuserede på problemstillingen; at forstå sætningen, fremfor at identificere sætningen – hvilket var en anderledes og ny vinkel at anskue anordningen fra. I dag anvendes stemmestyring i de fleste smartphones; herunder Android, iOS samt Windows Phone. Styresystemerne anvender stemmestyring som en integreret del, hvorved man kan foretage forskellige handlinger, såsom åbne at applikationer, tale-til-tekst, sætte alarmer samt navigationen uden brug af tastaturet. I takt med at stemmestyring er blevet mere alment tilgængeligt, er det nu også en integreret softwaredel i fjernsyn\cite{SpeechSamsung}, hvor brugeren har mulighed for at tænde, slukke samt skifte kanaler vha. stemmestyring. Det bedste og mest udviklede software indenfor stemmestyring er Dragon, som er en videreudvikling af IBM og Bell Laboratories' teknologi, og dermed den teknologi som har haft flest år på banen.\cite{SpeechMIT} I takt med at teknologien blomster, og elektroniske apparaturer bliver mere og mere avanceret, er der ligeledes sket en mærkværdig udvikling på stemmestyrings området. 

\subsubsection{Teknologisk gennemgang}
Stemmestyrings-controllere har flere intrikate\footnote{intrikate: \textit{indviklet; vanskelig at løse ofte pga. pinagtige omstændigheder}\cite{DDOintrikate}} trin der skal gennemføres, før at inputtet, altså stemmen, kan fortolkes af controlleren, og føre til en handling. Første og fremmest indsamles der digitale stikprøver, der måler på stemmens vibrationer, hvilket måles henover regulære tidsintervaller. En analog-digital konverter omsætter sproget til binær kode, som computeren kan forstå (figur \ref{fig:Analog}). Herefter analyseres den binære kode for uønsket støj, kategoriserer frekvenserne samt normaliserer lydhastigheden – hvilket er relevant, da mennesker snakker i vidt forskellige niveauer og hastigheder. Herefter splittes inputtet i 100-1000 dele af sekunder (afhængig af den pågældende controller). Det splittede input analyseres nu yderligere, hvor der undersøges for konsonanternes lyde som ’p’ og ’t’, hvilket sammenlignes med sprogets fonemer. Afslutningsvis konfereres fonemerne i en kontekstuel sammenhæng, hvor dataene gennemløbes i en kompleks statistisk model, som kigger på eksisterende ord, talemåder og sætninger. Med dette gennemløb af handlinger, dechifreres\footnote{dechifreres: \textit{analysere og forstå}\cite{DDOdechifrere}} brugerens input og hvis talen blev tydet korrekt, oversættes det til skrift, handling eller måske noget helt tredje\cite{SpeechMIT}.\\\\
\figur{Figurer/speech-recognition-sample.png}{Her ses en sammenhæng mellem stikprøvens præcision, altså inputtet, og den samlede kvalitet af outputtet \cite{SpeechHowStuffWorks}}{Analog}

\subsubsection{Problematikker forbundet med stemmestyring}
\label{sec:problemer_med_stemmestyring}
Der er flere problematikker forbundet med stemmestyrings-controllere; baggrundsstøj er en notabel fejlmargin, som kan betyde at inputtet ikke forstås korrekt og dermed foretages den ønskede handling ikke. I situationer hvor flere stemmer overlapper hinanden, som til møder, vil stemme-controlleren ikke filtrere inputtet – dette er også problematisk idet de mange stemmer alle skal analyseres og segmenteres, hvilket kræver meget processorkraft.\\
Sætninger, ord og dialekter kan alle være syndere til, at stemmestyringscontrollere ikke kan forstå inputtet. Flere ord staves og udtales ens, men bruges forskelligt i forhold til sammenhængen de er sagt i. Desto større databaser stemmestyringscontrolleren skal lede i, desto mere præcis vil resultatet også være. Databaserne udvides hele tiden, og controllerne har derved en markant højere sandsynlighed for at forstå inputtet korrekt, end hvad der var tilfældet for blot et par år tilbage \cite{SpeechMIT}\cite{SpeechHowStuffWorks}.